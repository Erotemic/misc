Neural Tangent Kernel
---------------------

https://arxiv.org/pdf/1806.07572

https://en.wikipedia.org/wiki/Neural_tangent_kernel

https://lilianweng.github.io/posts/2022-09-08-ntk/

https://www.borealisai.com/research-blogs/the-neural-tangent-kernel/

* when neural networks become very wide, their parameters do not change much during training and they can be considered as approximately linear

     + This makes sense because you can always rewrite a separable non-linear function as a linear function in higher dimensions [ref: https://stats.stackexchange.com/questions/479817/will-non-linear-data-always-become-linear-in-high-dimension]
